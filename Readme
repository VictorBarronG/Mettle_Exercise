# How to Run Locally

To run this Streamlit app locally, follow these steps:

1. Install the required packages by running `pip install -r requirements.txt`.
2. Install Ollama by following the instructions on the [Ollama GitHub page](https://github.com/hwchase17/langchain-ollama).
3. Download the `nomic-embed-text` model by running `ollama pull nomic-embed-text`.
4. Download the `llama3.1` model by running `ollama pull llama3.1`.
5. Once all dependencies are installed, run the Streamlit app by running `streamlit run app.py`.
6. The first time you run the app, it may take 5-8 minutes to create the vector store, depending on your system resources.
7. Once the app is running, you can interact with the chat window by asking a question.
8. The app may take a few minutes to respond, depending on the complexity of the question.

I have also included a notebook (`notebook.ipynb`) that you can use to run the functions separately and explore the relevant code chunks.

![alt text](image.png)
